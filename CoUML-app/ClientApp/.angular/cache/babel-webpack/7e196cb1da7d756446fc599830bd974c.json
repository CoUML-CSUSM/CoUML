{"ast":null,"code":"const pako = require('pako');\n\nconst {\n  copyObject,\n  parseOpId,\n  equalBytes\n} = require('../src/common');\n\nconst {\n  utf8ToString,\n  hexStringToBytes,\n  bytesToHexString,\n  Encoder,\n  Decoder,\n  RLEEncoder,\n  RLEDecoder,\n  DeltaEncoder,\n  DeltaDecoder,\n  BooleanEncoder,\n  BooleanDecoder\n} = require('./encoding'); // Maybe we should be using the platform's built-in hash implementation?\n// Node has the crypto module: https://nodejs.org/api/crypto.html and browsers have\n// https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto/digest\n// However, the WebCrypto API is asynchronous (returns promises), which would\n// force all our APIs to become asynchronous as well, which would be annoying.\n//\n// I think on balance, it's safe enough to use a random library off npm:\n// - We only need one hash function (not a full suite of crypto algorithms);\n// - SHA256 is quite simple and has fairly few opportunities for subtle bugs\n//   (compared to asymmetric cryptography anyway);\n// - It does not need a secure source of random bits and does not need to be\n//   constant-time;\n// - I have reviewed the source code and it seems pretty reasonable.\n\n\nconst {\n  Hash\n} = require('fast-sha256'); // These bytes don't mean anything, they were generated randomly\n\n\nconst MAGIC_BYTES = new Uint8Array([0x85, 0x6f, 0x4a, 0x83]);\nconst CHUNK_TYPE_DOCUMENT = 0;\nconst CHUNK_TYPE_CHANGE = 1;\nconst CHUNK_TYPE_DEFLATE = 2; // like CHUNK_TYPE_CHANGE but with DEFLATE compression\n// Minimum number of bytes in a value before we enable DEFLATE compression (there is no point\n// compressing very short values since compression may actually make them bigger)\n\nconst DEFLATE_MIN_SIZE = 256; // The least-significant 3 bits of a columnId indicate its datatype\n\nconst COLUMN_TYPE = {\n  GROUP_CARD: 0,\n  ACTOR_ID: 1,\n  INT_RLE: 2,\n  INT_DELTA: 3,\n  BOOLEAN: 4,\n  STRING_RLE: 5,\n  VALUE_LEN: 6,\n  VALUE_RAW: 7\n}; // The 4th-least-significant bit of a columnId is set if the column is DEFLATE-compressed\n\nconst COLUMN_TYPE_DEFLATE = 8; // In the values in a column of type VALUE_LEN, the bottom four bits indicate the type of the value,\n// one of the following types in VALUE_TYPE. The higher bits indicate the length of the value in the\n// associated VALUE_RAW column (in bytes).\n\nconst VALUE_TYPE = {\n  NULL: 0,\n  FALSE: 1,\n  TRUE: 2,\n  LEB128_UINT: 3,\n  LEB128_INT: 4,\n  IEEE754: 5,\n  UTF8: 6,\n  BYTES: 7,\n  COUNTER: 8,\n  TIMESTAMP: 9,\n  MIN_UNKNOWN: 10,\n  MAX_UNKNOWN: 15\n}; // make* actions must be at even-numbered indexes in this list\n\nconst ACTIONS = ['makeMap', 'set', 'makeList', 'del', 'makeText', 'inc', 'makeTable', 'link'];\nconst OBJECT_TYPE = {\n  makeMap: 'map',\n  makeList: 'list',\n  makeText: 'text',\n  makeTable: 'table'\n};\nconst COMMON_COLUMNS = [{\n  columnName: 'objActor',\n  columnId: 0 << 4 | COLUMN_TYPE.ACTOR_ID\n}, {\n  columnName: 'objCtr',\n  columnId: 0 << 4 | COLUMN_TYPE.INT_RLE\n}, {\n  columnName: 'keyActor',\n  columnId: 1 << 4 | COLUMN_TYPE.ACTOR_ID\n}, {\n  columnName: 'keyCtr',\n  columnId: 1 << 4 | COLUMN_TYPE.INT_DELTA\n}, {\n  columnName: 'keyStr',\n  columnId: 1 << 4 | COLUMN_TYPE.STRING_RLE\n}, {\n  columnName: 'idActor',\n  columnId: 2 << 4 | COLUMN_TYPE.ACTOR_ID\n}, {\n  columnName: 'idCtr',\n  columnId: 2 << 4 | COLUMN_TYPE.INT_DELTA\n}, {\n  columnName: 'insert',\n  columnId: 3 << 4 | COLUMN_TYPE.BOOLEAN\n}, {\n  columnName: 'action',\n  columnId: 4 << 4 | COLUMN_TYPE.INT_RLE\n}, {\n  columnName: 'valLen',\n  columnId: 5 << 4 | COLUMN_TYPE.VALUE_LEN\n}, {\n  columnName: 'valRaw',\n  columnId: 5 << 4 | COLUMN_TYPE.VALUE_RAW\n}, {\n  columnName: 'chldActor',\n  columnId: 6 << 4 | COLUMN_TYPE.ACTOR_ID\n}, {\n  columnName: 'chldCtr',\n  columnId: 6 << 4 | COLUMN_TYPE.INT_DELTA\n}];\nconst CHANGE_COLUMNS = COMMON_COLUMNS.concat([{\n  columnName: 'predNum',\n  columnId: 7 << 4 | COLUMN_TYPE.GROUP_CARD\n}, {\n  columnName: 'predActor',\n  columnId: 7 << 4 | COLUMN_TYPE.ACTOR_ID\n}, {\n  columnName: 'predCtr',\n  columnId: 7 << 4 | COLUMN_TYPE.INT_DELTA\n}]);\nconst DOC_OPS_COLUMNS = COMMON_COLUMNS.concat([{\n  columnName: 'succNum',\n  columnId: 8 << 4 | COLUMN_TYPE.GROUP_CARD\n}, {\n  columnName: 'succActor',\n  columnId: 8 << 4 | COLUMN_TYPE.ACTOR_ID\n}, {\n  columnName: 'succCtr',\n  columnId: 8 << 4 | COLUMN_TYPE.INT_DELTA\n}]);\nconst DOCUMENT_COLUMNS = [{\n  columnName: 'actor',\n  columnId: 0 << 4 | COLUMN_TYPE.ACTOR_ID\n}, {\n  columnName: 'seq',\n  columnId: 0 << 4 | COLUMN_TYPE.INT_DELTA\n}, {\n  columnName: 'maxOp',\n  columnId: 1 << 4 | COLUMN_TYPE.INT_DELTA\n}, {\n  columnName: 'time',\n  columnId: 2 << 4 | COLUMN_TYPE.INT_DELTA\n}, {\n  columnName: 'message',\n  columnId: 3 << 4 | COLUMN_TYPE.STRING_RLE\n}, {\n  columnName: 'depsNum',\n  columnId: 4 << 4 | COLUMN_TYPE.GROUP_CARD\n}, {\n  columnName: 'depsIndex',\n  columnId: 4 << 4 | COLUMN_TYPE.INT_DELTA\n}, {\n  columnName: 'extraLen',\n  columnId: 5 << 4 | COLUMN_TYPE.VALUE_LEN\n}, {\n  columnName: 'extraRaw',\n  columnId: 5 << 4 | COLUMN_TYPE.VALUE_RAW\n}];\n/**\n * Maps an opId of the form {counter: 12345, actorId: 'someActorId'} to the form\n * {counter: 12345, actorNum: 123, actorId: 'someActorId'}, where the actorNum\n * is the index into the `actorIds` array.\n */\n\nfunction actorIdToActorNum(opId, actorIds) {\n  if (!opId || !opId.actorId) return opId;\n  const counter = opId.counter;\n  const actorNum = actorIds.indexOf(opId.actorId);\n  if (actorNum < 0) throw new RangeError('missing actorId'); // should not happen\n\n  return {\n    counter,\n    actorNum,\n    actorId: opId.actorId\n  };\n}\n/**\n * Comparison function to pass to Array.sort(), which compares two opIds in the\n * form produced by `actorIdToActorNum` so that they are sorted in increasing\n * Lamport timestamp order (sorted first by counter, then by actorId).\n */\n\n\nfunction compareParsedOpIds(id1, id2) {\n  if (id1.counter < id2.counter) return -1;\n  if (id1.counter > id2.counter) return +1;\n  if (id1.actorId < id2.actorId) return -1;\n  if (id1.actorId > id2.actorId) return +1;\n  return 0;\n}\n/**\n * Takes `changes`, an array of changes (represented as JS objects). Returns an\n * object `{changes, actorIds}`, where `changes` is a copy of the argument in\n * which all string opIds have been replaced with `{counter, actorNum}` objects,\n * and where `actorIds` is a lexicographically sorted array of actor IDs occurring\n * in any of the operations. `actorNum` is an index into that array of actorIds.\n * If `single` is true, the actorId of the author of the change is moved to the\n * beginning of the array of actorIds, so that `actorNum` is zero when referencing\n * the author of the change itself. This special-casing is omitted if `single` is\n * false.\n */\n\n\nfunction parseAllOpIds(changes, single) {\n  const actors = {},\n        newChanges = [];\n\n  for (let change of changes) {\n    change = copyObject(change);\n    actors[change.actor] = true;\n    change.ops = expandMultiOps(change.ops, change.startOp, change.actor);\n    change.ops = change.ops.map(op => {\n      op = copyObject(op);\n      if (op.obj !== '_root') op.obj = parseOpId(op.obj);\n      if (op.elemId && op.elemId !== '_head') op.elemId = parseOpId(op.elemId);\n      if (op.child) op.child = parseOpId(op.child);\n      if (op.pred) op.pred = op.pred.map(parseOpId);\n      if (op.obj.actorId) actors[op.obj.actorId] = true;\n      if (op.elemId && op.elemId.actorId) actors[op.elemId.actorId] = true;\n      if (op.child && op.child.actorId) actors[op.child.actorId] = true;\n\n      for (let pred of op.pred) actors[pred.actorId] = true;\n\n      return op;\n    });\n    newChanges.push(change);\n  }\n\n  let actorIds = Object.keys(actors).sort();\n\n  if (single) {\n    actorIds = [changes[0].actor].concat(actorIds.filter(actor => actor !== changes[0].actor));\n  }\n\n  for (let change of newChanges) {\n    change.actorNum = actorIds.indexOf(change.actor);\n\n    for (let i = 0; i < change.ops.length; i++) {\n      let op = change.ops[i];\n      op.id = {\n        counter: change.startOp + i,\n        actorNum: change.actorNum,\n        actorId: change.actor\n      };\n      op.obj = actorIdToActorNum(op.obj, actorIds);\n      op.elemId = actorIdToActorNum(op.elemId, actorIds);\n      op.child = actorIdToActorNum(op.child, actorIds);\n      op.pred = op.pred.map(pred => actorIdToActorNum(pred, actorIds));\n    }\n  }\n\n  return {\n    changes: newChanges,\n    actorIds\n  };\n}\n/**\n * Encodes the `obj` property of operation `op` into the two columns\n * `objActor` and `objCtr`.\n */\n\n\nfunction encodeObjectId(op, columns) {\n  if (op.obj === '_root') {\n    columns.objActor.appendValue(null);\n    columns.objCtr.appendValue(null);\n  } else if (op.obj.actorNum >= 0 && op.obj.counter > 0) {\n    columns.objActor.appendValue(op.obj.actorNum);\n    columns.objCtr.appendValue(op.obj.counter);\n  } else {\n    throw new RangeError(`Unexpected objectId reference: ${JSON.stringify(op.obj)}`);\n  }\n}\n/**\n * Encodes the `key` and `elemId` properties of operation `op` into the three\n * columns `keyActor`, `keyCtr`, and `keyStr`.\n */\n\n\nfunction encodeOperationKey(op, columns) {\n  if (op.key) {\n    columns.keyActor.appendValue(null);\n    columns.keyCtr.appendValue(null);\n    columns.keyStr.appendValue(op.key);\n  } else if (op.elemId === '_head' && op.insert) {\n    columns.keyActor.appendValue(null);\n    columns.keyCtr.appendValue(0);\n    columns.keyStr.appendValue(null);\n  } else if (op.elemId && op.elemId.actorNum >= 0 && op.elemId.counter > 0) {\n    columns.keyActor.appendValue(op.elemId.actorNum);\n    columns.keyCtr.appendValue(op.elemId.counter);\n    columns.keyStr.appendValue(null);\n  } else {\n    throw new RangeError(`Unexpected operation key: ${JSON.stringify(op)}`);\n  }\n}\n/**\n * Encodes the `action` property of operation `op` into the `action` column.\n */\n\n\nfunction encodeOperationAction(op, columns) {\n  const actionCode = ACTIONS.indexOf(op.action);\n\n  if (actionCode >= 0) {\n    columns.action.appendValue(actionCode);\n  } else if (typeof op.action === 'number') {\n    columns.action.appendValue(op.action);\n  } else {\n    throw new RangeError(`Unexpected operation action: ${op.action}`);\n  }\n}\n/**\n * Given the datatype for a number, determine the typeTag and the value to encode\n * otherwise guess\n */\n\n\nfunction getNumberTypeAndValue(op) {\n  switch (op.datatype) {\n    case \"counter\":\n      return [VALUE_TYPE.COUNTER, op.value];\n\n    case \"timestamp\":\n      return [VALUE_TYPE.TIMESTAMP, op.value];\n\n    case \"uint\":\n      return [VALUE_TYPE.LEB128_UINT, op.value];\n\n    case \"int\":\n      return [VALUE_TYPE.LEB128_INT, op.value];\n\n    case \"float64\":\n      {\n        const buf64 = new ArrayBuffer(8),\n              view64 = new DataView(buf64);\n        view64.setFloat64(0, op.value, true);\n        return [VALUE_TYPE.IEEE754, new Uint8Array(buf64)];\n      }\n\n    default:\n      // increment operators get resolved here ...\n      if (Number.isInteger(op.value) && op.value <= Number.MAX_SAFE_INTEGER && op.value >= Number.MIN_SAFE_INTEGER) {\n        return [VALUE_TYPE.LEB128_INT, op.value];\n      } else {\n        const buf64 = new ArrayBuffer(8),\n              view64 = new DataView(buf64);\n        view64.setFloat64(0, op.value, true);\n        return [VALUE_TYPE.IEEE754, new Uint8Array(buf64)];\n      }\n\n  }\n}\n/**\n * Encodes the `value` property of operation `op` into the two columns\n * `valLen` and `valRaw`.\n */\n\n\nfunction encodeValue(op, columns) {\n  if (op.action !== 'set' && op.action !== 'inc' || op.value === null) {\n    columns.valLen.appendValue(VALUE_TYPE.NULL);\n  } else if (op.value === false) {\n    columns.valLen.appendValue(VALUE_TYPE.FALSE);\n  } else if (op.value === true) {\n    columns.valLen.appendValue(VALUE_TYPE.TRUE);\n  } else if (typeof op.value === 'string') {\n    const numBytes = columns.valRaw.appendRawString(op.value);\n    columns.valLen.appendValue(numBytes << 4 | VALUE_TYPE.UTF8);\n  } else if (ArrayBuffer.isView(op.value)) {\n    const numBytes = columns.valRaw.appendRawBytes(new Uint8Array(op.value.buffer));\n    columns.valLen.appendValue(numBytes << 4 | VALUE_TYPE.BYTES);\n  } else if (typeof op.value === 'number') {\n    let [typeTag, value] = getNumberTypeAndValue(op);\n    let numBytes;\n\n    if (typeTag === VALUE_TYPE.LEB128_UINT) {\n      numBytes = columns.valRaw.appendUint53(value);\n    } else if (typeTag === VALUE_TYPE.IEEE754) {\n      numBytes = columns.valRaw.appendRawBytes(value);\n    } else {\n      numBytes = columns.valRaw.appendInt53(value);\n    }\n\n    columns.valLen.appendValue(numBytes << 4 | typeTag);\n  } else if (typeof op.datatype === 'number' && op.datatype >= VALUE_TYPE.MIN_UNKNOWN && op.datatype <= VALUE_TYPE.MAX_UNKNOWN && op.value instanceof Uint8Array) {\n    const numBytes = columns.valRaw.appendRawBytes(op.value);\n    columns.valLen.appendValue(numBytes << 4 | op.datatype);\n  } else if (op.datatype) {\n    throw new RangeError(`Unknown datatype ${op.datatype} for value ${op.value}`);\n  } else {\n    throw new RangeError(`Unsupported value in operation: ${op.value}`);\n  }\n}\n/**\n * Given `sizeTag` (an unsigned integer read from a VALUE_LEN column) and `bytes` (a Uint8Array\n * read from a VALUE_RAW column, with length `sizeTag >> 4`), this function returns an object of the\n * form `{value: value, datatype: datatypeTag}` where `value` is a JavaScript primitive datatype\n * corresponding to the value, and `datatypeTag` is a datatype annotation such as 'counter'.\n */\n\n\nfunction decodeValue(sizeTag, bytes) {\n  if (sizeTag === VALUE_TYPE.NULL) {\n    return {\n      value: null\n    };\n  } else if (sizeTag === VALUE_TYPE.FALSE) {\n    return {\n      value: false\n    };\n  } else if (sizeTag === VALUE_TYPE.TRUE) {\n    return {\n      value: true\n    };\n  } else if (sizeTag % 16 === VALUE_TYPE.UTF8) {\n    return {\n      value: utf8ToString(bytes)\n    };\n  } else {\n    if (sizeTag % 16 === VALUE_TYPE.LEB128_UINT) {\n      return {\n        value: new Decoder(bytes).readUint53(),\n        datatype: \"uint\"\n      };\n    } else if (sizeTag % 16 === VALUE_TYPE.LEB128_INT) {\n      return {\n        value: new Decoder(bytes).readInt53(),\n        datatype: \"int\"\n      };\n    } else if (sizeTag % 16 === VALUE_TYPE.IEEE754) {\n      const view = new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength);\n\n      if (bytes.byteLength === 8) {\n        return {\n          value: view.getFloat64(0, true),\n          datatype: \"float64\"\n        };\n      } else {\n        throw new RangeError(`Invalid length for floating point number: ${bytes.byteLength}`);\n      }\n    } else if (sizeTag % 16 === VALUE_TYPE.COUNTER) {\n      return {\n        value: new Decoder(bytes).readInt53(),\n        datatype: 'counter'\n      };\n    } else if (sizeTag % 16 === VALUE_TYPE.TIMESTAMP) {\n      return {\n        value: new Decoder(bytes).readInt53(),\n        datatype: 'timestamp'\n      };\n    } else {\n      return {\n        value: bytes,\n        datatype: sizeTag % 16\n      };\n    }\n  }\n}\n/**\n * Reads one value from the column `columns[colIndex]` and interprets it based\n * on the column type. `actorIds` is a list of actors that appear in the change;\n * `actorIds[0]` is the actorId of the change's author. Mutates the `result`\n * object with the value, and returns the number of columns processed (this is 2\n * in the case of a pair of VALUE_LEN and VALUE_RAW columns, which are processed\n * in one go).\n */\n\n\nfunction decodeValueColumns(columns, colIndex, actorIds, result) {\n  const {\n    columnId,\n    columnName,\n    decoder\n  } = columns[colIndex];\n\n  if (columnId % 8 === COLUMN_TYPE.VALUE_LEN && colIndex + 1 < columns.length && columns[colIndex + 1].columnId === columnId + 1) {\n    const sizeTag = decoder.readValue();\n    const rawValue = columns[colIndex + 1].decoder.readRawBytes(sizeTag >> 4);\n    const {\n      value,\n      datatype\n    } = decodeValue(sizeTag, rawValue);\n    result[columnName] = value;\n    if (datatype) result[columnName + '_datatype'] = datatype;\n    return 2;\n  } else if (columnId % 8 === COLUMN_TYPE.ACTOR_ID) {\n    const actorNum = decoder.readValue();\n\n    if (actorNum === null) {\n      result[columnName] = null;\n    } else {\n      if (!actorIds[actorNum]) throw new RangeError(`No actor index ${actorNum}`);\n      result[columnName] = actorIds[actorNum];\n    }\n  } else {\n    result[columnName] = decoder.readValue();\n  }\n\n  return 1;\n}\n/**\n * Encodes an array of operations in a set of columns. The operations need to\n * be parsed with `parseAllOpIds()` beforehand. If `forDocument` is true, we use\n * the column structure of a whole document, otherwise we use the column\n * structure for an individual change. Returns an array of\n * `{columnId, columnName, encoder}` objects.\n */\n\n\nfunction encodeOps(ops, forDocument) {\n  const columns = {\n    objActor: new RLEEncoder('uint'),\n    objCtr: new RLEEncoder('uint'),\n    keyActor: new RLEEncoder('uint'),\n    keyCtr: new DeltaEncoder(),\n    keyStr: new RLEEncoder('utf8'),\n    insert: new BooleanEncoder(),\n    action: new RLEEncoder('uint'),\n    valLen: new RLEEncoder('uint'),\n    valRaw: new Encoder(),\n    chldActor: new RLEEncoder('uint'),\n    chldCtr: new DeltaEncoder()\n  };\n\n  if (forDocument) {\n    columns.idActor = new RLEEncoder('uint');\n    columns.idCtr = new DeltaEncoder();\n    columns.succNum = new RLEEncoder('uint');\n    columns.succActor = new RLEEncoder('uint');\n    columns.succCtr = new DeltaEncoder();\n  } else {\n    columns.predNum = new RLEEncoder('uint');\n    columns.predCtr = new DeltaEncoder();\n    columns.predActor = new RLEEncoder('uint');\n  }\n\n  for (let op of ops) {\n    encodeObjectId(op, columns);\n    encodeOperationKey(op, columns);\n    columns.insert.appendValue(!!op.insert);\n    encodeOperationAction(op, columns);\n    encodeValue(op, columns);\n\n    if (op.child && op.child.counter) {\n      columns.chldActor.appendValue(op.child.actorNum);\n      columns.chldCtr.appendValue(op.child.counter);\n    } else {\n      columns.chldActor.appendValue(null);\n      columns.chldCtr.appendValue(null);\n    }\n\n    if (forDocument) {\n      columns.idActor.appendValue(op.id.actorNum);\n      columns.idCtr.appendValue(op.id.counter);\n      columns.succNum.appendValue(op.succ.length);\n      op.succ.sort(compareParsedOpIds);\n\n      for (let i = 0; i < op.succ.length; i++) {\n        columns.succActor.appendValue(op.succ[i].actorNum);\n        columns.succCtr.appendValue(op.succ[i].counter);\n      }\n    } else {\n      columns.predNum.appendValue(op.pred.length);\n      op.pred.sort(compareParsedOpIds);\n\n      for (let i = 0; i < op.pred.length; i++) {\n        columns.predActor.appendValue(op.pred[i].actorNum);\n        columns.predCtr.appendValue(op.pred[i].counter);\n      }\n    }\n  }\n\n  let columnList = [];\n\n  for (let {\n    columnName,\n    columnId\n  } of forDocument ? DOC_OPS_COLUMNS : CHANGE_COLUMNS) {\n    if (columns[columnName]) columnList.push({\n      columnId,\n      columnName,\n      encoder: columns[columnName]\n    });\n  }\n\n  return columnList.sort((a, b) => a.columnId - b.columnId);\n}\n\nfunction validDatatype(value, datatype) {\n  if (datatype === undefined) {\n    return typeof value === 'string' || typeof value === 'boolean' || value === null;\n  } else {\n    return typeof value === 'number';\n  }\n}\n\nfunction expandMultiOps(ops, startOp, actor) {\n  let opNum = startOp;\n  let expandedOps = [];\n\n  for (const op of ops) {\n    if (op.action === 'set' && op.values && op.insert) {\n      if (op.pred.length !== 0) throw new RangeError('multi-insert pred must be empty');\n      let lastElemId = op.elemId;\n      const datatype = op.datatype;\n\n      for (const value of op.values) {\n        if (!validDatatype(value, datatype)) throw new RangeError(`Decode failed: bad value/datatype association (${value},${datatype})`);\n        expandedOps.push({\n          action: 'set',\n          obj: op.obj,\n          elemId: lastElemId,\n          datatype,\n          value,\n          pred: [],\n          insert: true\n        });\n        lastElemId = `${opNum}@${actor}`;\n        opNum += 1;\n      }\n    } else if (op.action === 'del' && op.multiOp > 1) {\n      if (op.pred.length !== 1) throw new RangeError('multiOp deletion must have exactly one pred');\n      const startElemId = parseOpId(op.elemId),\n            startPred = parseOpId(op.pred[0]);\n\n      for (let i = 0; i < op.multiOp; i++) {\n        const elemId = `${startElemId.counter + i}@${startElemId.actorId}`;\n        const pred = [`${startPred.counter + i}@${startPred.actorId}`];\n        expandedOps.push({\n          action: 'del',\n          obj: op.obj,\n          elemId,\n          pred\n        });\n        opNum += 1;\n      }\n    } else {\n      expandedOps.push(op);\n      opNum += 1;\n    }\n  }\n\n  return expandedOps;\n}\n/**\n * Takes a change as decoded by `decodeColumns`, and changes it into the form\n * expected by the rest of the backend. If `forDocument` is true, we use the op\n * structure of a whole document, otherwise we use the op structure for an\n * individual change.\n */\n\n\nfunction decodeOps(ops, forDocument) {\n  const newOps = [];\n\n  for (let op of ops) {\n    const obj = op.objCtr === null ? '_root' : `${op.objCtr}@${op.objActor}`;\n    const elemId = op.keyStr ? undefined : op.keyCtr === 0 ? '_head' : `${op.keyCtr}@${op.keyActor}`;\n    const action = ACTIONS[op.action] || op.action;\n    const newOp = elemId ? {\n      obj,\n      elemId,\n      action\n    } : {\n      obj,\n      key: op.keyStr,\n      action\n    };\n    newOp.insert = !!op.insert;\n\n    if (ACTIONS[op.action] === 'set' || ACTIONS[op.action] === 'inc') {\n      newOp.value = op.valLen;\n      if (op.valLen_datatype) newOp.datatype = op.valLen_datatype;\n    }\n\n    if (!!op.chldCtr !== !!op.chldActor) {\n      throw new RangeError(`Mismatched child columns: ${op.chldCtr} and ${op.chldActor}`);\n    }\n\n    if (op.chldCtr !== null) newOp.child = `${op.chldCtr}@${op.chldActor}`;\n\n    if (forDocument) {\n      newOp.id = `${op.idCtr}@${op.idActor}`;\n      newOp.succ = op.succNum.map(succ => `${succ.succCtr}@${succ.succActor}`);\n      checkSortedOpIds(op.succNum.map(succ => ({\n        counter: succ.succCtr,\n        actorId: succ.succActor\n      })));\n    } else {\n      newOp.pred = op.predNum.map(pred => `${pred.predCtr}@${pred.predActor}`);\n      checkSortedOpIds(op.predNum.map(pred => ({\n        counter: pred.predCtr,\n        actorId: pred.predActor\n      })));\n    }\n\n    newOps.push(newOp);\n  }\n\n  return newOps;\n}\n/**\n * Throws an exception if the opIds in the given array are not in sorted order.\n */\n\n\nfunction checkSortedOpIds(opIds) {\n  let last = null;\n\n  for (let opId of opIds) {\n    if (last && compareParsedOpIds(last, opId) !== -1) {\n      throw new RangeError('operation IDs are not in ascending order');\n    }\n\n    last = opId;\n  }\n}\n\nfunction encoderByColumnId(columnId) {\n  if ((columnId & 7) === COLUMN_TYPE.INT_DELTA) {\n    return new DeltaEncoder();\n  } else if ((columnId & 7) === COLUMN_TYPE.BOOLEAN) {\n    return new BooleanEncoder();\n  } else if ((columnId & 7) === COLUMN_TYPE.STRING_RLE) {\n    return new RLEEncoder('utf8');\n  } else if ((columnId & 7) === COLUMN_TYPE.VALUE_RAW) {\n    return new Encoder();\n  } else {\n    return new RLEEncoder('uint');\n  }\n}\n\nfunction decoderByColumnId(columnId, buffer) {\n  if ((columnId & 7) === COLUMN_TYPE.INT_DELTA) {\n    return new DeltaDecoder(buffer);\n  } else if ((columnId & 7) === COLUMN_TYPE.BOOLEAN) {\n    return new BooleanDecoder(buffer);\n  } else if ((columnId & 7) === COLUMN_TYPE.STRING_RLE) {\n    return new RLEDecoder('utf8', buffer);\n  } else if ((columnId & 7) === COLUMN_TYPE.VALUE_RAW) {\n    return new Decoder(buffer);\n  } else {\n    return new RLEDecoder('uint', buffer);\n  }\n}\n\nfunction makeDecoders(columns, columnSpec) {\n  const emptyBuf = new Uint8Array(0);\n  let decoders = [],\n      columnIndex = 0,\n      specIndex = 0;\n\n  while (columnIndex < columns.length || specIndex < columnSpec.length) {\n    if (columnIndex === columns.length || specIndex < columnSpec.length && columnSpec[specIndex].columnId < columns[columnIndex].columnId) {\n      const {\n        columnId,\n        columnName\n      } = columnSpec[specIndex];\n      decoders.push({\n        columnId,\n        columnName,\n        decoder: decoderByColumnId(columnId, emptyBuf)\n      });\n      specIndex++;\n    } else if (specIndex === columnSpec.length || columns[columnIndex].columnId < columnSpec[specIndex].columnId) {\n      const {\n        columnId,\n        buffer\n      } = columns[columnIndex];\n      decoders.push({\n        columnId,\n        decoder: decoderByColumnId(columnId, buffer)\n      });\n      columnIndex++;\n    } else {\n      // columns[columnIndex].columnId === columnSpec[specIndex].columnId\n      const {\n        columnId,\n        buffer\n      } = columns[columnIndex],\n            {\n        columnName\n      } = columnSpec[specIndex];\n      decoders.push({\n        columnId,\n        columnName,\n        decoder: decoderByColumnId(columnId, buffer)\n      });\n      columnIndex++;\n      specIndex++;\n    }\n  }\n\n  return decoders;\n}\n\nfunction decodeColumns(columns, actorIds, columnSpec) {\n  columns = makeDecoders(columns, columnSpec);\n  let parsedRows = [];\n\n  while (columns.some(col => !col.decoder.done)) {\n    let row = {},\n        col = 0;\n\n    while (col < columns.length) {\n      const columnId = columns[col].columnId;\n      let groupId = columnId >> 4,\n          groupCols = 1;\n\n      while (col + groupCols < columns.length && columns[col + groupCols].columnId >> 4 === groupId) {\n        groupCols++;\n      }\n\n      if (columnId % 8 === COLUMN_TYPE.GROUP_CARD) {\n        const values = [],\n              count = columns[col].decoder.readValue();\n\n        for (let i = 0; i < count; i++) {\n          let value = {};\n\n          for (let colOffset = 1; colOffset < groupCols; colOffset++) {\n            decodeValueColumns(columns, col + colOffset, actorIds, value);\n          }\n\n          values.push(value);\n        }\n\n        row[columns[col].columnName] = values;\n        col += groupCols;\n      } else {\n        col += decodeValueColumns(columns, col, actorIds, row);\n      }\n    }\n\n    parsedRows.push(row);\n  }\n\n  return parsedRows;\n}\n\nfunction decodeColumnInfo(decoder) {\n  // A number that is all 1 bits except for the bit that indicates whether a column is\n  // deflate-compressed. We ignore this bit when checking whether columns are sorted by ID.\n  const COLUMN_ID_MASK = (-1 ^ COLUMN_TYPE_DEFLATE) >>> 0;\n  let lastColumnId = -1,\n      columns = [],\n      numColumns = decoder.readUint53();\n\n  for (let i = 0; i < numColumns; i++) {\n    const columnId = decoder.readUint53(),\n          bufferLen = decoder.readUint53();\n\n    if ((columnId & COLUMN_ID_MASK) <= (lastColumnId & COLUMN_ID_MASK)) {\n      throw new RangeError('Columns must be in ascending order');\n    }\n\n    lastColumnId = columnId;\n    columns.push({\n      columnId,\n      bufferLen\n    });\n  }\n\n  return columns;\n}\n\nfunction encodeColumnInfo(encoder, columns) {\n  const nonEmptyColumns = columns.filter(column => column.encoder.buffer.byteLength > 0);\n  encoder.appendUint53(nonEmptyColumns.length);\n\n  for (let column of nonEmptyColumns) {\n    encoder.appendUint53(column.columnId);\n    encoder.appendUint53(column.encoder.buffer.byteLength);\n  }\n}\n\nfunction decodeChangeHeader(decoder) {\n  const numDeps = decoder.readUint53(),\n        deps = [];\n\n  for (let i = 0; i < numDeps; i++) {\n    deps.push(bytesToHexString(decoder.readRawBytes(32)));\n  }\n\n  let change = {\n    actor: decoder.readHexString(),\n    seq: decoder.readUint53(),\n    startOp: decoder.readUint53(),\n    time: decoder.readInt53(),\n    message: decoder.readPrefixedString(),\n    deps\n  };\n  const actorIds = [change.actor],\n        numActorIds = decoder.readUint53();\n\n  for (let i = 0; i < numActorIds; i++) actorIds.push(decoder.readHexString());\n\n  change.actorIds = actorIds;\n  return change;\n}\n/**\n * Assembles a chunk of encoded data containing a checksum, headers, and a\n * series of encoded columns. Calls `encodeHeaderCallback` with an encoder that\n * should be used to add the headers. The columns should be given as `columns`.\n */\n\n\nfunction encodeContainer(chunkType, encodeContentsCallback) {\n  const CHECKSUM_SIZE = 4; // checksum is first 4 bytes of SHA-256 hash of the rest of the data\n\n  const HEADER_SPACE = MAGIC_BYTES.byteLength + CHECKSUM_SIZE + 1 + 5; // 1 byte type + 5 bytes length\n\n  const body = new Encoder(); // Make space for the header at the beginning of the body buffer. We will\n  // copy the header in here later. This is cheaper than copying the body since\n  // the body is likely to be much larger than the header.\n\n  body.appendRawBytes(new Uint8Array(HEADER_SPACE));\n  encodeContentsCallback(body);\n  const bodyBuf = body.buffer;\n  const header = new Encoder();\n  header.appendByte(chunkType);\n  header.appendUint53(bodyBuf.byteLength - HEADER_SPACE); // Compute the hash over chunkType, length, and body\n\n  const headerBuf = header.buffer;\n  const sha256 = new Hash();\n  sha256.update(headerBuf);\n  sha256.update(bodyBuf.subarray(HEADER_SPACE));\n  const hash = sha256.digest(),\n        checksum = hash.subarray(0, CHECKSUM_SIZE); // Copy header into the body buffer so that they are contiguous\n\n  bodyBuf.set(MAGIC_BYTES, HEADER_SPACE - headerBuf.byteLength - CHECKSUM_SIZE - MAGIC_BYTES.byteLength);\n  bodyBuf.set(checksum, HEADER_SPACE - headerBuf.byteLength - CHECKSUM_SIZE);\n  bodyBuf.set(headerBuf, HEADER_SPACE - headerBuf.byteLength);\n  return {\n    hash,\n    bytes: bodyBuf.subarray(HEADER_SPACE - headerBuf.byteLength - CHECKSUM_SIZE - MAGIC_BYTES.byteLength)\n  };\n}\n\nfunction decodeContainerHeader(decoder, computeHash) {\n  if (!equalBytes(decoder.readRawBytes(MAGIC_BYTES.byteLength), MAGIC_BYTES)) {\n    throw new RangeError('Data does not begin with magic bytes 85 6f 4a 83');\n  }\n\n  const expectedHash = decoder.readRawBytes(4);\n  const hashStartOffset = decoder.offset;\n  const chunkType = decoder.readByte();\n  const chunkLength = decoder.readUint53();\n  const header = {\n    chunkType,\n    chunkLength,\n    chunkData: decoder.readRawBytes(chunkLength)\n  };\n\n  if (computeHash) {\n    const sha256 = new Hash();\n    sha256.update(decoder.buf.subarray(hashStartOffset, decoder.offset));\n    const binaryHash = sha256.digest();\n\n    if (!equalBytes(binaryHash.subarray(0, 4), expectedHash)) {\n      throw new RangeError('checksum does not match data');\n    }\n\n    header.hash = bytesToHexString(binaryHash);\n  }\n\n  return header;\n}\n\nfunction encodeChange(changeObj) {\n  const {\n    changes,\n    actorIds\n  } = parseAllOpIds([changeObj], true);\n  const change = changes[0];\n  const {\n    hash,\n    bytes\n  } = encodeContainer(CHUNK_TYPE_CHANGE, encoder => {\n    if (!Array.isArray(change.deps)) throw new TypeError('deps is not an array');\n    encoder.appendUint53(change.deps.length);\n\n    for (let hash of change.deps.slice().sort()) {\n      encoder.appendRawBytes(hexStringToBytes(hash));\n    }\n\n    encoder.appendHexString(change.actor);\n    encoder.appendUint53(change.seq);\n    encoder.appendUint53(change.startOp);\n    encoder.appendInt53(change.time);\n    encoder.appendPrefixedString(change.message || '');\n    encoder.appendUint53(actorIds.length - 1);\n\n    for (let actor of actorIds.slice(1)) encoder.appendHexString(actor);\n\n    const columns = encodeOps(change.ops, false);\n    encodeColumnInfo(encoder, columns);\n\n    for (let column of columns) encoder.appendRawBytes(column.encoder.buffer);\n\n    if (change.extraBytes) encoder.appendRawBytes(change.extraBytes);\n  });\n  const hexHash = bytesToHexString(hash);\n\n  if (changeObj.hash && changeObj.hash !== hexHash) {\n    throw new RangeError(`Change hash does not match encoding: ${changeObj.hash} != ${hexHash}`);\n  }\n\n  return bytes.byteLength >= DEFLATE_MIN_SIZE ? deflateChange(bytes) : bytes;\n}\n\nfunction decodeChangeColumns(buffer) {\n  if (buffer[8] === CHUNK_TYPE_DEFLATE) buffer = inflateChange(buffer);\n  const decoder = new Decoder(buffer);\n  const header = decodeContainerHeader(decoder, true);\n  const chunkDecoder = new Decoder(header.chunkData);\n  if (!decoder.done) throw new RangeError('Encoded change has trailing data');\n  if (header.chunkType !== CHUNK_TYPE_CHANGE) throw new RangeError(`Unexpected chunk type: ${header.chunkType}`);\n  const change = decodeChangeHeader(chunkDecoder);\n  const columns = decodeColumnInfo(chunkDecoder);\n\n  for (let i = 0; i < columns.length; i++) {\n    if ((columns[i].columnId & COLUMN_TYPE_DEFLATE) !== 0) {\n      throw new RangeError('change must not contain deflated columns');\n    }\n\n    columns[i].buffer = chunkDecoder.readRawBytes(columns[i].bufferLen);\n  }\n\n  if (!chunkDecoder.done) {\n    const restLen = chunkDecoder.buf.byteLength - chunkDecoder.offset;\n    change.extraBytes = chunkDecoder.readRawBytes(restLen);\n  }\n\n  change.columns = columns;\n  change.hash = header.hash;\n  return change;\n}\n/**\n * Decodes one change in binary format into its JS object representation.\n */\n\n\nfunction decodeChange(buffer) {\n  const change = decodeChangeColumns(buffer);\n  change.ops = decodeOps(decodeColumns(change.columns, change.actorIds, CHANGE_COLUMNS), false);\n  delete change.actorIds;\n  delete change.columns;\n  return change;\n}\n/**\n * Decodes the header fields of a change in binary format, but does not decode\n * the operations. Saves work when we only need to inspect the headers. Only\n * computes the hash of the change if `computeHash` is true.\n */\n\n\nfunction decodeChangeMeta(buffer, computeHash) {\n  if (buffer[8] === CHUNK_TYPE_DEFLATE) buffer = inflateChange(buffer);\n  const header = decodeContainerHeader(new Decoder(buffer), computeHash);\n\n  if (header.chunkType !== CHUNK_TYPE_CHANGE) {\n    throw new RangeError('Buffer chunk type is not a change');\n  }\n\n  const meta = decodeChangeHeader(new Decoder(header.chunkData));\n  meta.change = buffer;\n  if (computeHash) meta.hash = header.hash;\n  return meta;\n}\n/**\n * Compresses a binary change using DEFLATE.\n */\n\n\nfunction deflateChange(buffer) {\n  const header = decodeContainerHeader(new Decoder(buffer), false);\n  if (header.chunkType !== CHUNK_TYPE_CHANGE) throw new RangeError(`Unexpected chunk type: ${header.chunkType}`);\n  const compressed = pako.deflateRaw(header.chunkData);\n  const encoder = new Encoder();\n  encoder.appendRawBytes(buffer.subarray(0, 8)); // copy MAGIC_BYTES and checksum\n\n  encoder.appendByte(CHUNK_TYPE_DEFLATE);\n  encoder.appendUint53(compressed.byteLength);\n  encoder.appendRawBytes(compressed);\n  return encoder.buffer;\n}\n/**\n * Decompresses a binary change that has been compressed with DEFLATE.\n */\n\n\nfunction inflateChange(buffer) {\n  const header = decodeContainerHeader(new Decoder(buffer), false);\n  if (header.chunkType !== CHUNK_TYPE_DEFLATE) throw new RangeError(`Unexpected chunk type: ${header.chunkType}`);\n  const decompressed = pako.inflateRaw(header.chunkData);\n  const encoder = new Encoder();\n  encoder.appendRawBytes(buffer.subarray(0, 8)); // copy MAGIC_BYTES and checksum\n\n  encoder.appendByte(CHUNK_TYPE_CHANGE);\n  encoder.appendUint53(decompressed.byteLength);\n  encoder.appendRawBytes(decompressed);\n  return encoder.buffer;\n}\n/**\n * Takes an Uint8Array that may contain multiple concatenated changes, and\n * returns an array of subarrays, each subarray containing one change.\n */\n\n\nfunction splitContainers(buffer) {\n  let decoder = new Decoder(buffer),\n      chunks = [],\n      startOffset = 0;\n\n  while (!decoder.done) {\n    decodeContainerHeader(decoder, false);\n    chunks.push(buffer.subarray(startOffset, decoder.offset));\n    startOffset = decoder.offset;\n  }\n\n  return chunks;\n}\n/**\n * Decodes a list of changes from the binary format into JS objects.\n * `binaryChanges` is an array of `Uint8Array` objects.\n */\n\n\nfunction decodeChanges(binaryChanges) {\n  let decoded = [];\n\n  for (let binaryChange of binaryChanges) {\n    for (let chunk of splitContainers(binaryChange)) {\n      if (chunk[8] === CHUNK_TYPE_DOCUMENT) {\n        decoded = decoded.concat(decodeDocument(chunk));\n      } else if (chunk[8] === CHUNK_TYPE_CHANGE || chunk[8] === CHUNK_TYPE_DEFLATE) {\n        decoded.push(decodeChange(chunk));\n      } else {// ignoring chunk of unknown type\n      }\n    }\n  }\n\n  return decoded;\n}\n\nfunction sortOpIds(a, b) {\n  if (a === b) return 0;\n  if (a === '_root') return -1;\n  if (b === '_root') return +1;\n  const a_ = parseOpId(a),\n        b_ = parseOpId(b);\n  if (a_.counter < b_.counter) return -1;\n  if (a_.counter > b_.counter) return +1;\n  if (a_.actorId < b_.actorId) return -1;\n  if (a_.actorId > b_.actorId) return +1;\n  return 0;\n}\n/**\n * Takes a set of operations `ops` loaded from an encoded document, and\n * reconstructs the changes that they originally came from.\n * Does not return anything, only mutates `changes`.\n */\n\n\nfunction groupChangeOps(changes, ops) {\n  let changesByActor = {}; // map from actorId to array of changes by that actor\n\n  for (let change of changes) {\n    change.ops = [];\n    if (!changesByActor[change.actor]) changesByActor[change.actor] = [];\n\n    if (change.seq !== changesByActor[change.actor].length + 1) {\n      throw new RangeError(`Expected seq = ${changesByActor[change.actor].length + 1}, got ${change.seq}`);\n    }\n\n    if (change.seq > 1 && changesByActor[change.actor][change.seq - 2].maxOp > change.maxOp) {\n      throw new RangeError('maxOp must increase monotonically per actor');\n    }\n\n    changesByActor[change.actor].push(change);\n  }\n\n  let opsById = {};\n\n  for (let op of ops) {\n    if (op.action === 'del') throw new RangeError('document should not contain del operations');\n    op.pred = opsById[op.id] ? opsById[op.id].pred : [];\n    opsById[op.id] = op;\n\n    for (let succ of op.succ) {\n      if (!opsById[succ]) {\n        if (op.elemId) {\n          const elemId = op.insert ? op.id : op.elemId;\n          opsById[succ] = {\n            id: succ,\n            action: 'del',\n            obj: op.obj,\n            elemId,\n            pred: []\n          };\n        } else {\n          opsById[succ] = {\n            id: succ,\n            action: 'del',\n            obj: op.obj,\n            key: op.key,\n            pred: []\n          };\n        }\n      }\n\n      opsById[succ].pred.push(op.id);\n    }\n\n    delete op.succ;\n  }\n\n  for (let op of Object.values(opsById)) {\n    if (op.action === 'del') ops.push(op);\n  }\n\n  for (let op of ops) {\n    const {\n      counter,\n      actorId\n    } = parseOpId(op.id);\n    const actorChanges = changesByActor[actorId]; // Binary search to find the change that should contain this operation\n\n    let left = 0,\n        right = actorChanges.length;\n\n    while (left < right) {\n      const index = Math.floor((left + right) / 2);\n\n      if (actorChanges[index].maxOp < counter) {\n        left = index + 1;\n      } else {\n        right = index;\n      }\n    }\n\n    if (left >= actorChanges.length) {\n      throw new RangeError(`Operation ID ${op.id} outside of allowed range`);\n    }\n\n    actorChanges[left].ops.push(op);\n  }\n\n  for (let change of changes) {\n    change.ops.sort((op1, op2) => sortOpIds(op1.id, op2.id));\n    change.startOp = change.maxOp - change.ops.length + 1;\n    delete change.maxOp;\n\n    for (let i = 0; i < change.ops.length; i++) {\n      const op = change.ops[i],\n            expectedId = `${change.startOp + i}@${change.actor}`;\n\n      if (op.id !== expectedId) {\n        throw new RangeError(`Expected opId ${expectedId}, got ${op.id}`);\n      }\n\n      delete op.id;\n    }\n  }\n}\n\nfunction decodeDocumentChanges(changes, expectedHeads) {\n  let heads = {}; // change hashes that are not a dependency of any other change\n\n  for (let i = 0; i < changes.length; i++) {\n    let change = changes[i];\n    change.deps = [];\n\n    for (let index of change.depsNum.map(d => d.depsIndex)) {\n      if (!changes[index] || !changes[index].hash) {\n        throw new RangeError(`No hash for index ${index} while processing index ${i}`);\n      }\n\n      const hash = changes[index].hash;\n      change.deps.push(hash);\n      if (heads[hash]) delete heads[hash];\n    }\n\n    change.deps.sort();\n    delete change.depsNum;\n\n    if (change.extraLen_datatype !== VALUE_TYPE.BYTES) {\n      throw new RangeError(`Bad datatype for extra bytes: ${VALUE_TYPE.BYTES}`);\n    }\n\n    change.extraBytes = change.extraLen;\n    delete change.extraLen_datatype; // Encoding and decoding again to compute the hash of the change\n\n    changes[i] = decodeChange(encodeChange(change));\n    heads[changes[i].hash] = true;\n  }\n\n  const actualHeads = Object.keys(heads).sort();\n  let headsEqual = actualHeads.length === expectedHeads.length,\n      i = 0;\n\n  while (headsEqual && i < actualHeads.length) {\n    headsEqual = actualHeads[i] === expectedHeads[i];\n    i++;\n  }\n\n  if (!headsEqual) {\n    throw new RangeError(`Mismatched heads hashes: expected ${expectedHeads.join(', ')}, got ${actualHeads.join(', ')}`);\n  }\n}\n\nfunction encodeDocumentHeader(doc) {\n  const {\n    changesColumns,\n    opsColumns,\n    actorIds,\n    heads,\n    headsIndexes,\n    extraBytes\n  } = doc;\n\n  for (let column of changesColumns) deflateColumn(column);\n\n  for (let column of opsColumns) deflateColumn(column);\n\n  return encodeContainer(CHUNK_TYPE_DOCUMENT, encoder => {\n    encoder.appendUint53(actorIds.length);\n\n    for (let actor of actorIds) {\n      encoder.appendHexString(actor);\n    }\n\n    encoder.appendUint53(heads.length);\n\n    for (let head of heads.sort()) {\n      encoder.appendRawBytes(hexStringToBytes(head));\n    }\n\n    encodeColumnInfo(encoder, changesColumns);\n    encodeColumnInfo(encoder, opsColumns);\n\n    for (let column of changesColumns) encoder.appendRawBytes(column.encoder.buffer);\n\n    for (let column of opsColumns) encoder.appendRawBytes(column.encoder.buffer);\n\n    for (let index of headsIndexes) encoder.appendUint53(index);\n\n    if (extraBytes) encoder.appendRawBytes(extraBytes);\n  }).bytes;\n}\n\nfunction decodeDocumentHeader(buffer) {\n  const documentDecoder = new Decoder(buffer);\n  const header = decodeContainerHeader(documentDecoder, true);\n  const decoder = new Decoder(header.chunkData);\n  if (!documentDecoder.done) throw new RangeError('Encoded document has trailing data');\n  if (header.chunkType !== CHUNK_TYPE_DOCUMENT) throw new RangeError(`Unexpected chunk type: ${header.chunkType}`);\n  const actorIds = [],\n        numActors = decoder.readUint53();\n\n  for (let i = 0; i < numActors; i++) {\n    actorIds.push(decoder.readHexString());\n  }\n\n  const heads = [],\n        headsIndexes = [],\n        numHeads = decoder.readUint53();\n\n  for (let i = 0; i < numHeads; i++) {\n    heads.push(bytesToHexString(decoder.readRawBytes(32)));\n  }\n\n  const changesColumns = decodeColumnInfo(decoder);\n  const opsColumns = decodeColumnInfo(decoder);\n\n  for (let i = 0; i < changesColumns.length; i++) {\n    changesColumns[i].buffer = decoder.readRawBytes(changesColumns[i].bufferLen);\n    inflateColumn(changesColumns[i]);\n  }\n\n  for (let i = 0; i < opsColumns.length; i++) {\n    opsColumns[i].buffer = decoder.readRawBytes(opsColumns[i].bufferLen);\n    inflateColumn(opsColumns[i]);\n  }\n\n  if (!decoder.done) {\n    for (let i = 0; i < numHeads; i++) headsIndexes.push(decoder.readUint53());\n  }\n\n  const extraBytes = decoder.readRawBytes(decoder.buf.byteLength - decoder.offset);\n  return {\n    changesColumns,\n    opsColumns,\n    actorIds,\n    heads,\n    headsIndexes,\n    extraBytes\n  };\n}\n\nfunction decodeDocument(buffer) {\n  const {\n    changesColumns,\n    opsColumns,\n    actorIds,\n    heads\n  } = decodeDocumentHeader(buffer);\n  const changes = decodeColumns(changesColumns, actorIds, DOCUMENT_COLUMNS);\n  const ops = decodeOps(decodeColumns(opsColumns, actorIds, DOC_OPS_COLUMNS), true);\n  groupChangeOps(changes, ops);\n  decodeDocumentChanges(changes, heads);\n  return changes;\n}\n/**\n * DEFLATE-compresses the given column if it is large enough to make the compression worthwhile.\n */\n\n\nfunction deflateColumn(column) {\n  if (column.encoder.buffer.byteLength >= DEFLATE_MIN_SIZE) {\n    column.encoder = {\n      buffer: pako.deflateRaw(column.encoder.buffer)\n    };\n    column.columnId |= COLUMN_TYPE_DEFLATE;\n  }\n}\n/**\n * Decompresses the given column if it is DEFLATE-compressed.\n */\n\n\nfunction inflateColumn(column) {\n  if ((column.columnId & COLUMN_TYPE_DEFLATE) !== 0) {\n    column.buffer = pako.inflateRaw(column.buffer);\n    column.columnId ^= COLUMN_TYPE_DEFLATE;\n  }\n}\n\nmodule.exports = {\n  COLUMN_TYPE,\n  VALUE_TYPE,\n  ACTIONS,\n  OBJECT_TYPE,\n  DOC_OPS_COLUMNS,\n  CHANGE_COLUMNS,\n  DOCUMENT_COLUMNS,\n  encoderByColumnId,\n  decoderByColumnId,\n  makeDecoders,\n  decodeValue,\n  splitContainers,\n  encodeChange,\n  decodeChangeColumns,\n  decodeChange,\n  decodeChangeMeta,\n  decodeChanges,\n  encodeDocumentHeader,\n  decodeDocumentHeader,\n  decodeDocument\n};","map":null,"metadata":{},"sourceType":"script"}